# -*- coding: utf-8 -*-
"""Project2_glass.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ESVJGnrjDeyaSnYg1kvRxuUt4qhQI9_U
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import math
import itertools
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score, StratifiedKFold
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from imblearn.metrics import geometric_mean_score, specificity_score, sensitivity_score

# Glass Type Identification KEEL Dataset (~65%)
gls_df = pd.read_csv('Glass2.csv',names=["RI","Na","Mg","Al","Si","K","Ca","Ba","Fe","Class"],index_col= False) 

# Check for Dataset Imbalance and Plot Class Distribution
gls_total = gls_df["Class"]
gls_neg = gls_total.value_counts()[0]/len(gls_total)
gls_pos = gls_total.value_counts()[1]/len(gls_total)
sns.countplot('Class', data = gls_df)
plt.title('Glass Dataset Class Distribution')
plt.savefig('Glass_Imbalance_Dist.png', dpi=300)
print("Class '0': ",round(gls_neg,2),"%")
print("Class '1': ",round(gls_pos,2),"%")
print("Imbalance Ratio: ",round(gls_neg/gls_pos,2))

# Split dataset into 70/30 train-test ratio
x = ["RI","Na","Mg","Al","Si","K","Ca","Ba","Fe"]
y = ["Class"]
x_gls = gls_df[x]
y_gls = gls_df[y]
x_train, x_test, y_train, y_test = train_test_split(x_gls, y_gls, test_size=0.3)

# Desicion Tree Classifier
dt_gls = DecisionTreeClassifier().fit(x_train,y_train)
dt_scores = cross_val_score(dt_gls,x_test,y_test,cv=10)

# Random Forest Classifier
rf_gls = RandomForestClassifier().fit(x_train,y_train)
rf_scores = cross_val_score(rf_gls,x_test,y_test,cv=10)

# Decision Tree Classifier Baseline Score
print(dt_scores.mean())

# Random Forest Classifier Baseline Score
print(rf_scores.mean())

# Partition Data into 10 Bins
n_bins = 10
skf = StratifiedKFold(n_splits=n_bins, shuffle=True)

x_trainbins, x_testbins = [], []
y_trainbins, y_testbins = [], []

for train, test in skf.split(x_gls, y_gls):
  skf_xtrain, skf_xtest = x_gls.loc[train,:"Fe"], x_gls.loc[test,:"Fe"] 
  skf_ytrain, skf_ytest = y_gls.loc[train,:"Class"], y_gls.loc[test,:"Class"]
  x_trainbins.append(skf_xtrain), y_trainbins.append(skf_ytrain)
  x_testbins.append(skf_xtest), y_testbins.append(skf_ytest)

# Metrics 
scores = []
sense = []
specs = []
youden = []
gmean = []

# Permutate throughout 10 bins
numbers = [0,1,2,3,4,5,6,7,8,9]
perm = list(itertools.combinations(numbers,9))

comb = []
for i in range(0,10):
  comb.append(perm[i])
  comb.append(list(set(numbers) - set(perm[i]))[0])

x_cluster = []
for i in comb[0]:
  x_cluster.append(x_trainbins[i])
  x_cluster[i]['Class'] = y_trainbins[i]
x_cluster.append(x_trainbins[comb[1]])
x_cluster[9]['Class'] = y_trainbins[comb[1]]

x_cluster.insert(1, x_cluster.pop(0))

# Elbow Method for k
for i in range(0,9):
  sse = []
  for k in range(1,10):
      km = KMeans(n_clusters=k).fit(x_cluster[i])
      sse.append(km.inertia_)

  plt.figure(0)
  plt.plot(range(1,10), sse)
  plt.title('Elbow Method')
  plt.xlabel('k')
  plt.ylabel('Sum of Squared Distance')
  plt.savefig('Elbow_Glass.png', dpi=300)

# Silhouette Method for k
for i in range(0,9):
  sil = []
  for k in range(2, 10):
    kmeans = KMeans(n_clusters = k).fit(x_cluster[i])
    labels = kmeans.labels_
    sil.append(silhouette_score(x_cluster[i], labels, metric = 'euclidean'))

  plt.figure(1)
  plt.bar(range(2,10), sil)
  plt.title('Silhouette Method')
  plt.xlabel('k')
  plt.ylabel('Silhouette Score')
  plt.savefig('Silhouette_Glass.png', dpi=300)

# K-means with correct k
num_clusters = 3
minority = (x_cluster[i]["Class"].values == 1).sum()
min_class = []
labels = []
center = []
for i in range(0,9):
  model = KMeans(n_clusters=num_clusters).fit(x_cluster[i])
  labels.append(model.labels_)
  center.append(model.cluster_centers_)
  min_class.append(minority)

# Assign X to Closest Cluster
y_pred = model.predict(x_cluster[9])
score = geometric_mean_score(x_cluster[9]['Class'],y_pred,average=None)
sensitivity = sensitivity_score(x_cluster[9]['Class'],y_pred,average='micro')
specificity = specificity_score(x_cluster[9]['Class'],y_pred,average='micro')
scores.append(score)
sense.append(sensitivity)
specs.append(specificity)
youden.append((sensitivity + specificity - 1))
gmean.append(math.sqrt(specificity*sensitivity))

# Compute Metric Scores
x_dp = sensitivity / (1-sensitivity)
y_dp = specificity / (1-specificity)
dp = (math.sqrt(3)/math.pi)
print("Sensitivity ", sensitivity)
print("Specificity ", specificity)
print("Youden's Index Score: ", (sensitivity + specificity - 1))
print('Geometric Mean Score: ',math.sqrt(specificity*sensitivity))

# Model Final Score
print("Youden's Index Mean Score: ", np.mean(youden))
print("Youden's Index Standard Deviation : ", np.std(youden))
plt.figure()
plt.subplot(221)
plt.boxplot(youden)
plt.title("Youden's Index")
plt.ylabel('Score')
print("Geometric Mean Score: ", np.mean(gmean))
print("Geometric Mean Standard Deviation : ", np.std(gmean))
plt.subplot(222)
plt.boxplot(gmean)
plt.title("Geometric Mean")
print("Sensitivity Mean Score: ", np.mean(sense))
print("Sensitivity Standard Deviation : ", np.std(sense))
plt.subplot(223)
plt.boxplot(sense)
plt.title("Sensitivity")
plt.ylabel('Score')
print("Specificity Mean Score: ", np.mean(specs))
print("Specificity Standard Deviation : ", np.std(specs))
plt.subplot(224)
plt.boxplot(specs)
plt.title("Specificity")
plt.savefig('Metrics_Glass.png', dpi=300)
plt.show()